# Results Directory Structure

This directory contains organized benchmark results with auto-incrementing run numbers.

## Directory Layout

```
results/
├── token_reduction/         # Token reduction benchmarks
│   ├── run_001_results.csv
│   ├── run_001_summary.json
│   ├── run_002_results.csv
│   └── ...
├── llm_comprehension/       # LLM comprehension tests
│   ├── run_001_gpt35turbo.json
│   ├── run_002_gpt4o.json
│   └── ...
└── visualizations/          # Generated charts
    ├── run_001_token_reduction.png
    ├── run_001_similarity_tradeoff.png
    └── ...
```

## Run Numbering

- Each benchmark type maintains its own auto-incrementing counter
- Runs are numbered sequentially: `run_001`, `run_002`, etc.
- Filenames include identifiers for the test type or model used

## Token Reduction Benchmarks

Generated by `scripts/benchmark.py`:
- `run_XXX_results.csv` - Detailed per-file, per-strategy results
- `run_XXX_summary.json` - Aggregate statistics

## LLM Comprehension Benchmarks

Generated by `scripts/llm_benchmark.py`:
- `run_XXX_<model>.json` - Full Q&A results with comprehension metrics
- Includes: questions, answers (original vs minified), token savings, similarity scores

## Visualizations

Generated by `scripts/visualize.py` from token reduction benchmarks:
- `run_XXX_token_reduction.png` - Bar chart of reduction by strategy
- `run_XXX_similarity_tradeoff.png` - Scatter plot of reduction vs similarity
- `run_XXX_per_file_analysis.png` - Detailed per-file breakdown
- `run_XXX_processing_time.png` - Performance comparison

## Usage

All scripts automatically use the next available run number:

```bash
# Token reduction benchmark (auto-numbered)
python3 scripts/benchmark.py testdata/samples

# Or specify custom output
python3 scripts/benchmark.py testdata/samples --output custom/path.csv

# LLM comprehension test (auto-numbered)
python3 scripts/llm_benchmark.py testdata/samples --model gpt-4o

# Visualize latest token reduction results
python3 scripts/visualize.py
```
